<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Emotion Detection Model Interface</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <link
            href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap"
            rel="stylesheet"
        />
        <style>
            body {
                font-family: "Inter", sans-serif;
                background-color: #f7f7f7;
            }
        </style>
    </head>
    <body class="bg-gray-100 min-h-screen flex items-center justify-center p-4">
        <div
            class="w-full max-w-2xl bg-white shadow-xl rounded-2xl p-8 space-y-8"
        >
            <header class="text-center">
                <h1 class="text-3xl font-bold text-gray-800">
                    Facial Emotion Detector (ML Project)
                </h1>
                <p class="text-lg text-gray-500 mt-2">
                    Final Project for {{ '{{ model_status }}' }}
                </p>
            </header>

            <!-- Main Content Area: Webcam Feed and Prediction Output -->
            <main class="space-y-6">
                <div class="flex flex-col md:flex-row gap-6">
                    <!-- Webcam/Video Area -->
                    <div
                        class="flex-1 bg-gray-200 rounded-xl shadow-inner p-2 flex items-center justify-center h-64 md:h-80 relative"
                    >
                        <video
                            id="video"
                            class="rounded-lg w-full h-full object-cover"
                            autoplay
                            playsinline
                            muted
                        ></video>
                        <!-- Canvas used for drawing resized frames (hidden) -->
                        <canvas
                            id="capture-canvas"
                            width="48"
                            height="48"
                            style="display: none"
                        ></canvas>
                        <!-- Overlay for emotion text -->
                        <div
                            id="overlay"
                            class="absolute left-4 top-4 bg-black bg-opacity-50 text-white px-3 py-2 rounded-lg"
                        >
                            <div
                                id="emotion-output"
                                class="text-2xl font-extrabold"
                            >
                                Neutral
                            </div>
                            <div
                                id="confidence-output"
                                class="text-sm opacity-80"
                            >
                                Confidence: 0.00
                            </div>
                        </div>
                    </div>

                    <!-- Live Prediction Output -->
                    <div
                        class="md:w-1/3 bg-blue-500 text-white rounded-xl shadow-lg p-6 flex flex-col justify-center items-center text-center"
                    >
                        <h2 class="text-xl font-semibold mb-3">Live Emotion</h2>
                        <p
                            id="sidebar-emotion"
                            class="text-4xl font-extrabold tracking-tight"
                        >
                            Neutral
                        </p>
                        <p
                            id="sidebar-confidence"
                            class="text-sm opacity-80 mt-2"
                        >
                            Confidence: 0.00
                        </p>
                    </div>
                </div>

                <!-- Control Panel -->
                <div
                    class="bg-gray-50 p-4 rounded-xl shadow-md border border-gray-200"
                >
                    <h3 class="text-xl font-semibold text-gray-700 mb-3">
                        Controls
                    </h3>
                    <button
                        id="start-btn"
                        class="w-full px-4 py-2 bg-green-600 text-white font-medium rounded-lg hover:bg-green-700 transition duration-150 shadow-md"
                    >
                        Start Detection
                    </button>
                    <p class="text-sm text-gray-500 mt-2 text-center">
                        Click Start to activate the webcam and begin real-time
                        emotion analysis.
                    </p>
                </div>
            </main>

            <footer class="pt-4 border-t text-center text-sm text-gray-400">
                <p>&copy; 2024 Emotion Detection System</p>
            </footer>
        </div>

        <script>
            // Load OpenCV.js (Emscripten build). If you prefer a pinned version, replace the URL.
        </script>
        <script async src="https://docs.opencv.org/4.5.5/opencv.js"></script>
        <script>
            let opencvReady = false;
            if (typeof cv !== "undefined") {
                // If cv is already present (rare), mark ready
                opencvReady = true;
            }
            // OpenCV will call this when its WASM runtime is ready
            function onOpenCvReady() {
                opencvReady = true;
                console.log("OpenCV.js is ready");
            }
            if (typeof cv !== "undefined") {
                cv["onRuntimeInitialized"] = onOpenCvReady;
            } else {
                // Poll for cv in case script loads after this script block
                const cvCheck = setInterval(() => {
                    if (typeof cv !== "undefined") {
                        clearInterval(cvCheck);
                        cv["onRuntimeInitialized"] = onOpenCvReady;
                    }
                }, 200);
            }
            // Frontend webcam capture + frame sender
            const video = document.getElementById("video");
            const captureCanvas = document.getElementById("capture-canvas");
            const captureCtx = captureCanvas.getContext("2d");
            // Intermediate canvas to draw the full-size video frame for OpenCV processing
            const videoCanvas = document.createElement("canvas");
            videoCanvas.id = "video-canvas";
            videoCanvas.style.display = "none";
            document.body.appendChild(videoCanvas);
            const videoCtx = videoCanvas.getContext("2d");
            const overlayEmotion = document.getElementById("emotion-output");
            const overlayConfidence =
                document.getElementById("confidence-output");
            const sidebarEmotion = document.getElementById("sidebar-emotion");
            const sidebarConfidence =
                document.getElementById("sidebar-confidence");

            let streaming = false;
            let sendInterval = null; // interval handler

            async function startWebcam() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { facingMode: "user" },
                        audio: false,
                    });
                    video.srcObject = stream;
                    await video.play();
                    streaming = true;

                    // Start periodic capture and send (approx 4 FPS)
                    if (sendInterval) clearInterval(sendInterval);
                    sendInterval = setInterval(captureAndSendFrame, 250);
                } catch (err) {
                    console.error("Could not start webcam:", err);
                    alert(
                        "Webcam access is required for live emotion detection. Please allow camera permissions."
                    );
                }
            }

            async function captureAndSendFrame() {
                if (!streaming) return;

                try {
                    let dataUrl = null;
                    if (opencvReady && typeof cv !== "undefined") {
                        // Ensure video canvas matches current video dimensions
                        videoCanvas.width = video.videoWidth || 320;
                        videoCanvas.height = video.videoHeight || 240;
                        // Draw current frame to intermediate canvas
                        videoCtx.drawImage(
                            video,
                            0,
                            0,
                            videoCanvas.width,
                            videoCanvas.height
                        );

                        // Use OpenCV to convert to grayscale and resize to 48x48
                        let src = cv.imread(videoCanvas);
                        let gray = new cv.Mat();
                        cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY);
                        let dst = new cv.Mat();
                        let dsize = new cv.Size(
                            captureCanvas.width,
                            captureCanvas.height
                        );
                        cv.resize(gray, dst, dsize, 0, 0, cv.INTER_AREA);

                        // Show resized grayscale into captureCanvas
                        cv.imshow(captureCanvas, dst);

                        // release mats
                        src.delete();
                        gray.delete();
                        dst.delete();

                        // Now get a JPEG dataURL from the capture canvas
                        dataUrl = captureCanvas.toDataURL("image/jpeg", 0.8);
                    } else {
                        // Fallback: simple canvas resize (no explicit grayscale)
                        captureCtx.drawImage(
                            video,
                            0,
                            0,
                            captureCanvas.width,
                            captureCanvas.height
                        );
                        dataUrl = captureCanvas.toDataURL("image/jpeg", 0.7);
                    }

                    // POST to backend prediction endpoint
                    const resp = await fetch("/predict_emotion", {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify({ image_data: dataUrl }),
                    });

                    if (!resp.ok) {
                        // If server returns 503 or other errors, show message once
                        const err = await resp.json().catch(() => ({}));
                        console.warn("Prediction API error", resp.status, err);
                        return;
                    }

                    const j = await resp.json();
                    if (j && j.success) {
                        overlayEmotion.textContent = j.emotion;
                        overlayConfidence.textContent = `Confidence: ${j.confidence}`;
                        sidebarEmotion.textContent = j.emotion;
                        sidebarConfidence.textContent = `Confidence: ${j.confidence}`;
                    } else if (j && j.error) {
                        console.warn("Prediction error:", j.error);
                    }
                } catch (e) {
                    console.error("Error capturing or sending frame", e);
                }
            }

            document
                .getElementById("start-btn")
                .addEventListener("click", () => {
                    if (!streaming) {
                        overlayEmotion.textContent = "Loading...";
                        overlayConfidence.textContent = "Starting camera...";
                        startWebcam();
                    }
                });
        </script>
    </body>
</html>
